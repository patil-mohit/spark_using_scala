{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROCESSING COLUMN DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@46161c97\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@46161c97"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.\n",
    "    builder.\n",
    "    config(\"spark.ui.port\", \"2828\").\n",
    "    appName(\"Data Processing - Overview\").\n",
    "    master(\"yarn\").\n",
    "    getOrCreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@46161c97"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dummy data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "l = List(x)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "List(x)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val l = List(\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [dummy: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[dummy: string]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = l.toDF(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dummy: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    x|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the system DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|current_date()|\n",
      "+--------------+\n",
      "|    2020-05-19|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_date).\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|current_date|\n",
      "+------------+\n",
      "|  2020-05-19|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_date.alias(\"current_date\")).\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employees = List((1,Scott,Tiger,1000.0,united states,+1 123 456 7890,123 456 7890), (2,Henry,Ford,1250.0,India,+1 000 999 7890,000 999 7890), (3,Nick,Junior,750.0,united KINGDOM,+1 555 999 7890,666 888 7890), (4,Bill,Gomes,1500.0,AUSTRALIA,+1 234 900 7890,234 900 7890))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "List((1,Scott,Tiger,1000.0,united states,+1 123 456 7890,123 456 7890), (2,Henry,Ford,1250.0,India,+1 000 999 7890,000 999 7890), (3,Nick,Junior,750.0,united KINGDOM,+1 555 999 7890,666 888 7890), (4,Bill,Gomes,1500.0,AUSTRALIA,+1 234 900 7890,234 900 7890))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val employees = List((1, \"Scott\", \"Tiger\", 1000.0, \"united states\",\"+1 123 456 7890\", \"123 456 7890\"),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, \"India\",\"+1 000 999 7890\", \"000 999 7890\"),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, \"united KINGDOM\",\"+1 555 999 7890\", \"666 888 7890\"),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, \"AUSTRALIA\",\"+1 234 900 7890\", \"234 900 7890\")\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employeesDF = [employee_id: int, first_name: string ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[employee_id: int, first_name: string ... 5 more fields]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val employeesDF = employees.\n",
    "    toDF(\"employee_id\", \"first_name\",\"last_name\", \"salary\", \"nationality\",\"phone no\",\"ssn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emplyee_id: integer (nullable = false)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- salary: double (nullable = false)\n",
      " |-- nationality: string (nullable = true)\n",
      " |-- phone no: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emplyeesDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+------+--------------+---------------+------------+\n",
      "|emplyee_id|first_name|last_name|salary|nationality   |phone no       |ssn         |\n",
      "+----------+----------+---------+------+--------------+---------------+------------+\n",
      "|1         |Scott     |Tiger    |1000.0|united states |+1 123 456 7890|123 456 7890|\n",
      "|2         |Henry     |Ford     |1250.0|India         |+1 000 999 7890|000 999 7890|\n",
      "|3         |Nick      |Junior   |750.0 |united KINGDOM|+1 555 999 7890|666 888 7890|\n",
      "|4         |Bill      |Gomes    |1500.0|AUSTRALIA     |+1 234 900 7890|234 900 7890|\n",
      "+----------+----------+---------+------+--------------+---------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categories of functions\n",
    "\n",
    "There are approximately 300 functions under org.apache.spark.sql.function. \n",
    "At higher level they can be grouped into a few categoroes.\n",
    "\n",
    "1) String manipulation functions:\n",
    "- case conversion - `lower` , `upper`\n",
    "- Getting Length - `length'\n",
    "- Extracting substring - `substring`, `split`\n",
    "- Trimming - `trim`, `ltrim`, `rtrim`\n",
    "- Padding - `lpad`, `rpad`\n",
    "\n",
    "2) Data Manipulation functions:\n",
    "- Getting current date and time - `current_date`, `current_timestammp`\n",
    "- Data Arithmetic - `date_add`, `date_sub`, `datediff`, `months_between`, `add_months`,`next_day`\n",
    "- Begining and Ending Date or Time - `Last_day`, `trunc`,`date_trunc`\n",
    "- Formatting Date - `date_format`\n",
    "- Extracting Information - `dayofyear`,`dayofmonth`, `dayofweek`,`year`,`month`\n",
    "\n",
    "3) Aggregation functions:\n",
    "- `count`,`countDistinct`\n",
    "- `sum`, `avg`\n",
    "\n",
    "4) Special function\n",
    "- `col`,`lit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Function - col and lit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create Data Frame for demo purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employees = List((1,Scott,Tiger,1000.0,united states,+1 123 456 7890,123 456 7890), (2,Henry,Ford,1250.0,India,+1 000 999 7890,000 999 7890), (3,Nick,Junior,750.0,united KINGDOM,+1 555 999 7890,666 888 7890), (4,Bill,Gomes,1500.0,AUSTRALIA,+1 234 900 7890,234 900 7890))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "List((1,Scott,Tiger,1000.0,united states,+1 123 456 7890,123 456 7890), (2,Henry,Ford,1250.0,India,+1 000 999 7890,000 999 7890), (3,Nick,Junior,750.0,united KINGDOM,+1 555 999 7890,666 888 7890), (4,Bill,Gomes,1500.0,AUSTRALIA,+1 234 900 7890,234 900 7890))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val employees = List((1, \"Scott\", \"Tiger\", 1000.0, \"united states\",\"+1 123 456 7890\", \"123 456 7890\"),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, \"India\",\"+1 000 999 7890\", \"000 999 7890\"),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, \"united KINGDOM\",\"+1 555 999 7890\", \"666 888 7890\"),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, \"AUSTRALIA\",\"+1 234 900 7890\", \"234 900 7890\")\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employeesDF = [employee_id: int, first_name: string ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[employee_id: int, first_name: string ... 5 more fields]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val employeesDF = employees.\n",
    "     toDF(\"employee_id\", \"first_name\",\"last_name\", \"salary\", \"nationality\",\"phone no\",\"ssn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = false)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- salary: double (nullable = false)\n",
      " |-- nationality: string (nullable = true)\n",
      " |-- phone no: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF.show(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For Data Frame API such as `select`,`groupBy`,`orderBy` etc we can pass column names as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:29: error: stable identifier required, but this.$line7$read.spark.implicits found.\n",
       "       import spark.implicits._\n",
       "                    ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// to use perators such as $ in place of functions like col\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|Scott     |Tiger    |\n",
      "|Henry     |Ford     |\n",
      "|Nick      |Junior   |\n",
      "|Bill      |Gomes    |\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select($\"first_name\", $\"last_name\").\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Alternative using col function \n",
    "// $ is a shorthand operator for col from implicits\n",
    "import org.apache.spark.sql.functions.col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|     Scott|    Tiger|\n",
      "|     Henry|     Ford|\n",
      "|      Nick|   Junior|\n",
      "|      Bill|    Gomes|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select(col(\"first_name\"), col(\"last_name\")).\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|     Scott|    Tiger|\n",
      "|     Henry|     Ford|\n",
      "|      Nick|   Junior|\n",
      "|      Bill|    Gomes|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Alterative passing by column names as string\n",
    "// As long we donot need to invoke any function\n",
    "employeesDF.\n",
    "    select(\"first_name\", \"last_name\").\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Even if you want to apply fucntion for any one column. all the columns should be of col type not string type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:33: error: overloaded method value select with alternatives:\n",
       "  [U1, U2](c1: org.apache.spark.sql.TypedColumn[org.apache.spark.sql.Row,U1], c2: org.apache.spark.sql.TypedColumn[org.apache.spark.sql.Row,U2])org.apache.spark.sql.Dataset[(U1, U2)] <and>\n",
       "  (col: String,cols: String*)org.apache.spark.sql.DataFrame <and>\n",
       "  (cols: org.apache.spark.sql.Column*)org.apache.spark.sql.DataFrame\n",
       " cannot be applied to (String, org.apache.spark.sql.ColumnName)\n",
       "           select(\"first_name\",$\"last_name\").\n",
       "           ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select(\"first_name\",$\"last_name\").\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|     Scott|    Tiger|\n",
      "|     Henry|     Ford|\n",
      "|      Nick|   Junior|\n",
      "|      Bill|    Gomes|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select($\"first_name\", $\"last_name\").\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  If there are no transformations on any column in any function then we should be able to pass all column names as strings.\n",
    "-  If not we need to pass all column names as type column by using col fucntion or its shorthand operator $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Passing column as part of groupBy\n",
    "** PROBLEM STATEMENT: Get the count of the employees per nation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|nationality   |count|\n",
      "+--------------+-----+\n",
      "|India         |1    |\n",
      "|united KINGDOM|1    |\n",
      "|united states |1    |\n",
      "|AUSTRALIA     |1    |\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    groupBy(\"nationality\").\n",
    "    count.\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Upper case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|upper(nationality)|count|\n",
      "+------------------+-----+\n",
      "|    UNITED KINGDOM|    1|\n",
      "|             INDIA|    1|\n",
      "|         AUSTRALIA|    1|\n",
      "|     UNITED STATES|    1|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    groupBy(upper($\"nationality\")).\n",
    "    count.\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- passing columns as a part of orderBy or sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------+\n",
      "|function_desc                                                                             |\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|Function: concat                                                                          |\n",
      "|Class: org.apache.spark.sql.catalyst.expressions.Concat                                   |\n",
      "|Usage: concat(str1, str2, ..., strN) - Returns the concatenation of str1, str2, ..., strN.|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE FUNCTION concat\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW FUNCTIONS\").show(300,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+---------------+------------+\n",
      "|employee_id|first_name|last_name|salary|nationality   |phone no       |ssn         |\n",
      "+-----------+----------+---------+------+--------------+---------------+------------+\n",
      "|4          |Bill      |Gomes    |1500.0|AUSTRALIA     |+1 234 900 7890|234 900 7890|\n",
      "|2          |Henry     |Ford     |1250.0|India         |+1 000 999 7890|000 999 7890|\n",
      "|3          |Nick      |Junior   |750.0 |united KINGDOM|+1 555 999 7890|666 888 7890|\n",
      "|1          |Scott     |Tiger    |1000.0|united states |+1 123 456 7890|123 456 7890|\n",
      "+-----------+----------+---------+------+--------------+---------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    orderBy(upper($\"nationality\")).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+---------------+------------+\n",
      "|employee_id|first_name|last_name|salary|nationality   |phone no       |ssn         |\n",
      "+-----------+----------+---------+------+--------------+---------------+------------+\n",
      "|4          |Bill      |Gomes    |1500.0|AUSTRALIA     |+1 234 900 7890|234 900 7890|\n",
      "|2          |Henry     |Ford     |1250.0|India         |+1 000 999 7890|000 999 7890|\n",
      "|3          |Nick      |Junior   |750.0 |united KINGDOM|+1 555 999 7890|666 888 7890|\n",
      "|1          |Scott     |Tiger    |1000.0|united states |+1 123 456 7890|123 456 7890|\n",
      "+-----------+----------+---------+------+--------------+---------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Alternative - we can also refer column names using Data Frames like this\n",
    "\n",
    "employeesDF.\n",
    "    orderBy(upper(employeesDF(\"nationality\"))).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Smetimes, we want to add a literal to the column values. \n",
    "  -  for eaxmple - we might want to concatenate first_name and last_name with seperated by comma and space in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:47: error: type mismatch;\n",
       " found   : String(\", \")\n",
       " required: org.apache.spark.sql.Column\n",
       "           select(concat($\"first_name\",\", \", $\"last_name\")).\n",
       "                                       ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select(concat($\"first_name\",\", \", $\"last_name\")).\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Above code fails because we are passing string ',' which is treating as column here\n",
    "- All the 3 fields should be column type\n",
    "- However, ',' is a literal here and we have to use lit function to convert literal value to column type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{concat, lit}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|concat(first_name, ,, last_name)|\n",
      "+--------------------------------+\n",
      "|                     Scott,Tiger|\n",
      "|                      Henry,Ford|\n",
      "|                     Nick,Junior|\n",
      "|                      Bill,Gomes|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select(concat($\"first_name\", lit(\",\"), $\"last_name\")).\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|  full_name|\n",
      "+-----------+\n",
      "|Scott,Tiger|\n",
      "| Henry,Ford|\n",
      "|Nick,Junior|\n",
      "| Bill,Gomes|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select(concat($\"first_name\", lit(\",\"), $\"last_name\").alias(\"full_name\")).\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String manipulation - Case conversion and Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Convert all the alphabetic characters is a string to uppercase - `upper`\n",
    " - Convert all the alphabetic characters is a string to lowercase - `lower`\n",
    " - Convert first character in a string to uppercase - `initcap`\n",
    " - Get the number of characters in a string - `length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{upper, col, length, lower, initcap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-----------------+-----------------+-------------------+\n",
      "|employee_id|nationality   |nationality_upper|nationality_lower|nationality_initcap|\n",
      "+-----------+--------------+-----------------+-----------------+-------------------+\n",
      "|1          |united states |UNITED STATES    |united states    |13                 |\n",
      "|2          |India         |INDIA            |india            |5                  |\n",
      "|3          |united KINGDOM|UNITED KINGDOM   |united kingdom   |14                 |\n",
      "|4          |AUSTRALIA     |AUSTRALIA        |australia        |9                  |\n",
      "+-----------+--------------+-----------------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select(\"employee_id\",\"nationality\").\n",
    "    withColumn(\"nationality_upper\", upper($\"nationality\")).\n",
    "    withColumn(\"nationality_lower\", lower($\"nationality\")).\n",
    "    withColumn(\"nationality_initcap\", initcap($\"nationality\")).\n",
    "    withColumn(\"nationality_initcap\", length($\"nationality\")).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Manipulation - substring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us understand `substring` function\n",
    "- If we are processing <b>fixed length columns</b> then we use 'substring' to extract information\n",
    "- Example : extract last 4 digits from SSN\n",
    "- `substring` function takes 3 arguments, <b>column, position, length</b>. We can also provide position from the end by passing negative vaalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s = Hello World\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Hello"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val s = \"Hello World\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hello"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.substring(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ell"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.substring(1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "l = List(x)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "List(x)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val l = List(\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [dummy_col: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[dummy_col: string]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = l.toDF(\"dummy_col\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dummy_col: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|dummy_col|\n",
      "+---------+\n",
      "|        x|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|substring(Hello World, 7, 5)|\n",
      "+----------------------------+\n",
      "|                       World|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.\n",
    "    select(substring(lit(\"Hello World\"), 7, 5)).\n",
    "    show\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|substring(Hello World, -5, 5)|\n",
      "+-----------------------------+\n",
      "|                        World|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.\n",
    "    select(substring(lit(\"Hello World\"), -5, 5)).\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a list of employees with name, ssn and phone number\n",
    "- SSN format should be 3 2 4 - fixed length with 9 digits\n",
    "- Phone number format : Country code is variable and remaining phone number have 10 digits :\n",
    "-- Country code - 1 - 3 digits\n",
    "-- phone number prefix - 3 digits\n",
    "-- phone number remaining - 4 digits\n",
    "-- all the 4 parts are seperated by spaces\n",
    "- create Dataframe with column names, ssn and phone_number\n",
    "- Extract last 4 digits from phone number\n",
    "- Extract last 4 digits from SSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employees = List((1,Scott,Tiger,1000.0,united states,+11 23456 7890,1234567890), (2,Henry,Ford,1250.0,India,+10 00999 7890,0009997890), (3,Nick,Junior,750.0,united KINGDOM,+15 55999 7890,6668887890), (4,Bill,Gomes,1500.0,AUSTRALIA,+12 34900 7890,2349007890))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "List((1,Scott,Tiger,1000.0,united states,+11 23456 7890,1234567890), (2,Henry,Ford,1250.0,India,+10 00999 7890,0009997890), (3,Nick,Junior,750.0,united KINGDOM,+15 55999 7890,6668887890), (4,Bill,Gomes,1500.0,AUSTRALIA,+12 34900 7890,2349007890))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val employees = List((1, \"Scott\", \"Tiger\", 1000.0, \"united states\",\"+11 23456 7890\", \"1234567890\"),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, \"India\",\"+10 00999 7890\", \"0009997890\"),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, \"united KINGDOM\",\"+15 55999 7890\", \"6668887890\"),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, \"AUSTRALIA\",\"+12 34900 7890\", \"2349007890\")\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employeesDF = [employee_id: int, first_name: string ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[employee_id: int, first_name: string ... 5 more fields]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val employeesDF = employees.\n",
    "    toDF(\"employee_id\", \"first_name\",\"last_name\", \"salary\", \"nationality\",\"phone_no\",\"ssn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+----------+\n",
      "|employee_id|      phone_no|       ssn|\n",
      "+-----------+--------------+----------+\n",
      "|          1|+11 23456 7890|1234567890|\n",
      "|          2|+10 00999 7890|0009997890|\n",
      "|          3|+15 55999 7890|6668887890|\n",
      "|          4|+12 34900 7890|2349007890|\n",
      "+-----------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select(\"employee_id\", \"phone_no\",\"ssn\").\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+----------+-----------+------------+----------------+---------+\n",
      "|employee_id|      phone_no|       ssn|phone_last4|phone_prefix|phone_remaining4|ssn_last4|\n",
      "+-----------+--------------+----------+-----------+------------+----------------+---------+\n",
      "|          1|+11 23456 7890|1234567890|       7890|         +11|            7890|     7890|\n",
      "|          2|+10 00999 7890|0009997890|       7890|         +10|            7890|     7890|\n",
      "|          3|+15 55999 7890|6668887890|       7890|         +15|            7890|     7890|\n",
      "|          4|+12 34900 7890|2349007890|       7890|         +12|            7890|     7890|\n",
      "+-----------+--------------+----------+-----------+------------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select(\"employee_id\", \"phone_no\",\"ssn\").\n",
    "    withColumn(\"phone_last4\", substring($\"phone_no\", -4, 4).cast(\"int\")).\n",
    "    withColumn(\"phone_prefix\", substring($\"phone_no\", 1, 3)).\n",
    "    withColumn(\"phone_remaining4\", substring($\"phone_no\", -4, 4)).\n",
    "    withColumn(\"ssn_last4\", substring($\"ssn\", -4, 4).cast(\"int\")).\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+----------+-----------+------------+----------------+---------+\n",
      "|employee_id|      phone_no|       ssn|phone_last4|phone_prefix|phone_remaining4|ssn_last4|\n",
      "+-----------+--------------+----------+-----------+------------+----------------+---------+\n",
      "|          1|+11 23456 7890|1234567890|       7890|          11|            7890|     7890|\n",
      "|          2|+10 00999 7890|0009997890|       7890|          10|            7890|     7890|\n",
      "|          3|+15 55999 7890|6668887890|       7890|          15|            7890|     7890|\n",
      "|          4|+12 34900 7890|2349007890|       7890|          12|            7890|     7890|\n",
      "+-----------+--------------+----------+-----------+------------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Another way to do above tasks\n",
    "employeesDF.\n",
    "    select($\"employee_id\", $\"phone_no\", $\"ssn\",\n",
    "            substring($\"phone_no\",-4,4).cast(\"int\").alias(\"phone_last4\"),\n",
    "            substring($\"phone_no\",1,3).cast(\"int\").alias(\"phone_prefix\"),\n",
    "            substring($\"phone_no\", -4, 4).cast(\"int\").alias(\"phone_remaining4\"),\n",
    "            substring($\"ssn\", -4, 4).cast(\"int\").alias(\"ssn_last4\")\n",
    "          ).\n",
    "    show        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Manipulation - split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us understand how we can extract substring using `split`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we are processing <b>variable length columns</b> with <b>delimiter</b> the we can use `split` to extract the information\n",
    "- Here are some of the examples for variable length columns and the use cases for which we typically extract information.\n",
    " - Address where we store House Number, Street Name and City, state and Zip code comma seperated. We might want to extract city and state for demographic reports.\n",
    "- `split` takes 2 arguments, **column** and **delimiter**\n",
    "- `split` converts each string into array and we can access elements using index.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "l = List(x)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "List(x)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val l = List(\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [dummy: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[dummy: string]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = l.toDF(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{split,lit}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|split(Hello world, how are you,  )|\n",
      "+----------------------------------+\n",
      "|[Hello, world,, how, are, you]    |\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(split(lit(\"Hello world, how are you\"), \" \")).\n",
    "   show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+\n",
      "|split(Hello world, how are you,  )[2]|\n",
      "+-------------------------------------+\n",
      "|how                                  |\n",
      "+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(split(lit(\"Hello world, how are you\"), \" \")(2)).\n",
    "   show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+----------+-----------+\n",
      "|employee_id|phone_no      |ssn       |phone_last4|\n",
      "+-----------+--------------+----------+-----------+\n",
      "|1          |+11 23456 7890|1234567890|7890       |\n",
      "|2          |+10 00999 7890|0009997890|7890       |\n",
      "|3          |+15 55999 7890|6668887890|7890       |\n",
      "|4          |+12 34900 7890|2349007890|7890       |\n",
      "+-----------+--------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// using withColumn way\n",
    "employeesDF.\n",
    "        select(\"employee_id\", \"phone_no\",\"ssn\").\n",
    "        withColumn(\"phone_last4\", split($\"phone_no\", \" \")(2).cast(\"int\")).\n",
    "        show(false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+----------+-----------+\n",
      "|employee_id|phone_no      |ssn       |phone_last4|\n",
      "+-----------+--------------+----------+-----------+\n",
      "|1          |+11 23456 7890|1234567890|7890       |\n",
      "|2          |+10 00999 7890|0009997890|7890       |\n",
      "|3          |+15 55999 7890|6668887890|7890       |\n",
      "|4          |+12 34900 7890|2349007890|7890       |\n",
      "+-----------+--------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// with select way\n",
    "employeesDF.\n",
    "    select($\"employee_id\", $\"phone_no\", $\"ssn\",\n",
    "            split($\"phone_no\", \" \")(2).cast(\"int\").alias(\"phone_last4\")).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Manipulation - Cancatenation of String"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us understand how to concatenate strings using `concat` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - We can pass the variable number of strings to `concat` function\n",
    " - It will return one string concatenating all strings\n",
    " - If we have to concatenate literal in between the strings we have to use `lit` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+--------------+----------+----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|      phone_no|       ssn| full_name|\n",
      "+-----------+----------+---------+------+--------------+--------------+----------+----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states|+11 23456 7890|1234567890|ScottTiger|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+10 00999 7890|0009997890| HenryFord|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+15 55999 7890|6668887890|NickJunior|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+12 34900 7890|2349007890| BillGomes|\n",
      "+-----------+----------+---------+------+--------------+--------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    withColumn(\"full_name\", concat($\"first_name\", $\"last_name\")).\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{concat,lit}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+--------------+----------+-----------+\n",
      "|employee_id|first_name|last_name|salary|nationality   |phone_no      |ssn       |full_name  |\n",
      "+-----------+----------+---------+------+--------------+--------------+----------+-----------+\n",
      "|1          |Scott     |Tiger    |1000.0|united states |+11 23456 7890|1234567890|Scott Tiger|\n",
      "|2          |Henry     |Ford     |1250.0|India         |+10 00999 7890|0009997890|Henry Ford |\n",
      "|3          |Nick      |Junior   |750.0 |united KINGDOM|+15 55999 7890|6668887890|Nick Junior|\n",
      "|4          |Bill      |Gomes    |1500.0|AUSTRALIA     |+12 34900 7890|2349007890|Bill Gomes |\n",
      "+-----------+----------+---------+------+--------------+--------------+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "employeesDF.\n",
    "    withColumn(\"full_name\", concat($\"first_name\", lit(\" \"), $\"last_name\")).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Manipulation - Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Typically we pad characters to build fixed length value or recoreds\n",
    "- Fixed length values or records are extensively used in a mainframes based system\n",
    "- Length of each and every field in fixed length records is determined and if the value of the field is less than predetemined length then we pad with a standard character\n",
    "- In terms of numeric fields we pad with zero on leading or left side. For non numeric fields, we pad with some standard character on trailing or right side.\n",
    "- We use `lpad` to pad a string with a specific character on leading or left side and `rpad` to pad on trailing or right side.\n",
    "- Both lpad and rpad takes 3 arguments - column or expression, desired length and the character need to be paded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "l = List(x)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "List(x)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val l = List(\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [dummy: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[dummy: string]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = l.toDF(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{lit, lpad}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|dummy     |\n",
      "+----------+\n",
      "|-----Hello|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(lpad(lit(\"Hello\"), 10, \"-\").alias(\"dummy\")).show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|10   |\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(lpad(lit(100), 2, \"0\").alias(\"dummy\")).show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-------------+--------------+----------+\n",
      "|employee_id|first_name|last_name|salary|  nationality|      phone_no|       ssn|\n",
      "+-----------+----------+---------+------+-------------+--------------+----------+\n",
      "|          1|     Scott|    Tiger|1000.0|united states|+11 23456 7890|1234567890|\n",
      "|          2|     Henry|     Ford|1250.0|        India|+10 00999 7890|0009997890|\n",
      "+-----------+----------+---------+------+-------------+--------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks \n",
    "- Use `pad` function to convert each of the field into fixed length and concatenate.\n",
    " - Length of the employee_id should be 5 characters and should be padded with zero.\n",
    " - Length of first_name and last_name should be 10 characters and should be padded with - on right side\n",
    " - Length of salary should be 10 characters and should be padded with zero.\n",
    " - Length of nationality should be 15 characters and should be padded with - on right side\n",
    " - Length of phone_number should be 17 characters and should be padded with - on right side.\n",
    " - Length of ssn can be left as is. it is 17 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{lpad, rpad, concat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "empFixedDF = [employee: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[employee: string]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val empFixedDF = employeesDF.\n",
    "                select(\n",
    "                 concat(\n",
    "                     lpad($\"employee_id\", 5, \"0\"),\n",
    "                     rpad($\"first_name\", 10, \"-\"),\n",
    "                     rpad($\"last_name\", 10, \"-\"), \n",
    "                     lpad($\"salary\", 10, \"0\"),\n",
    "                     lpad($\"nationality\", 15, \"-\"),\n",
    "                     lpad($\"phone_no\", 17, \"-\")\n",
    "                 ).alias(\"employee\")\n",
    "                 )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------+\n",
      "|employee                                                           |\n",
      "+-------------------------------------------------------------------+\n",
      "|00001Scott-----Tiger-----00001000.0--united states---+11 23456 7890|\n",
      "|00002Henry-----Ford------00001250.0----------India---+10 00999 7890|\n",
      "|00003Nick------Junior----00000750.0-united KINGDOM---+15 55999 7890|\n",
      "|00004Bill------Gomes-----00001500.0------AUSTRALIA---+12 34900 7890|\n",
      "+-------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empFixedDF.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Manipulation - Triming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Let us understand how to trim unwanted and trailing characters around a string\n",
    "  - We typically use trimmimg to remove unnecessary character from fixed length records.\n",
    "  - Fixed length records are extensively used in Mainframes and we might have to process it using spark.\n",
    "  - As part of processing we might want to remove leading or trailing characters such as 0 in case of numeric types and spaces or some satndard charcter in case of alphanumeric types.\n",
    "  - As of now spark function takes the column as argument and remove leading or trailing spaces\n",
    "  - Trim spaces towards left - `ltrim`\n",
    "  - Trim spaces towards right - `rtrim`\n",
    "  - Trim spaces towards sides - `trim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{rtrim, trim, ltrim}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "l = List(\"    Hello.    \")\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "List(\"    Hello.    \")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val l = List(\"    Hello.    \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [dummy: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[dummy: string]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = l.toDF(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+---------+-----+\n",
      "|         dummy|     ltrim|    rtrim| trim|\n",
      "+--------------+----------+---------+-----+\n",
      "|    Hello.    |Hello.    |    Hello|Hello|\n",
      "+--------------+----------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " df.withColumn(\"ltrim\", ltrim($\"dummy\")).\n",
    "    withColumn(\"rtrim\", rtrim(rtrim($\"dummy\"),\".\")).\n",
    "    withColumn(\"trim\", trim(trim($\"dummy\"),\".\")).\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date and Time - Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let us overview about Date and Time using available functions.\n",
    " - We ca use `current_date` to get today's server date.\n",
    "  - Date will be returned uding **yyyy-MM-dd** format.\n",
    "  - We can use `current_timestamp` to get current server time.\n",
    "  - Timestamp will be returned using **yyyy-MM-dd HH:mm:ss.SSS** format\n",
    "  - Hours will be by default 24 hour format\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "val l = List(\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [dummy: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[dummy: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = l.toDF(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{current_date, current_timestamp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|current_date()|\n",
      "+--------------+\n",
      "|    2020-05-23|\n",
      "+--------------+\n"
     ]
    }
   ],
   "source": [
    "df.select(current_date).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|current_timestamp()    |\n",
      "+-----------------------+\n",
      "|2020-05-23 15:52:12.744|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_timestamp).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date and Time - Arithmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us perform Date and Time Arithmetic using relevant functions.\n",
    " \n",
    "- Adding days to a date or timestamp - `date_add`\n",
    "- Substracting days from a date or timestamp - `date_sub`\n",
    "- Getting difference between 2 dates or timestamp - `datediff`\n",
    "- Gwtting number of moths between 2 dates or timestamps - `months_between`\n",
    "- Adding months to a date or timestamp - `add_months`\n",
    "- Getting next day from given date - `next_day`\n",
    "- All the functions are self explainatory. We can apply these on standard date or timestamp. All the functions return date even when applied on timestamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetimes = List((2014-02-28,2014-02-28 10:00:00.123), (2016-02-29,2016-02-29 08:08:08.999), (2017-10-31,2017-12-31 11:59:59.123), (2019-11-30,2019-08-31 00:00:00.000))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "List((2014-02-28,2014-02-28 10:00:00.123), (2016-02-29,2016-02-29 08:08:08.999), (2017-10-31,2017-12-31 11:59:59.123), (2019-11-30,2019-08-31 00:00:00.000))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datetimes= List((\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "           (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "           (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "           (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetimesDF = [date: string, time: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[date: string, time: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datetimesDF = datetimes.toDF(\"date\", \"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|date      |time                   |\n",
      "+----------+-----------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|\n",
      "+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add 10 days to both date and time values\n",
    "- Substract 10 days from both date and time values\n",
    "- Get the difference between current_date and date values as well as current_timestamp abd time values.\n",
    "- Get the nnumber of months between current_date and date values as well as current_timestamp and time values\n",
    "- Add 3 onths to both date values as well as time values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{date_add, date_sub}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+----------+----------+----------+----------+\n",
      "|date      |time                   |date10    |time10    |subDate10 |subTime10 |\n",
      "+----------+-----------------------+----------+----------+----------+----------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014-03-10|2014-03-10|2014-02-18|2014-02-18|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016-03-10|2016-03-10|2016-02-19|2016-02-19|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017-11-10|2018-01-10|2017-10-21|2017-12-21|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019-12-10|2019-09-10|2019-11-20|2019-08-21|\n",
      "+----------+-----------------------+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.\n",
    "    select(\"date\",\"time\").\n",
    "    withColumn(\"date10\", date_add($\"date\", 10)).\n",
    "    withColumn(\"time10\", date_add($\"time\",10)).\n",
    "    withColumn(\"subDate10\", date_sub($\"date\",10)).\n",
    "    withColumn(\"subTime10\", date_sub($\"time\",10)).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{current_date, current_timestamp, datediff}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------+-------------+\n",
      "|date      |time                   |datediff_date|datediff_time|\n",
      "+----------+-----------------------+-------------+-------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2279         |2279         |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|1548         |1548         |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|938          |877          |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|178          |269          |\n",
      "+----------+-----------------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"datediff_date\", datediff(current_date, $\"date\")).\n",
    "    withColumn(\"datediff_time\", datediff(current_timestamp, $\"time\")).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{months_between, add_months, round}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------+-------------------+---------------+-------------+\n",
      "|date      |time                   |datediff_date|months_between_time|add_months_date|add_time_date|\n",
      "+----------+-----------------------+-------------+-------------------+---------------+-------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|74.94        |74.94              |2014-05-31     |2014-05-31   |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|50.9         |50.91              |2016-05-31     |2016-05-31   |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|30.84        |28.84              |2018-01-31     |2018-03-31   |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|5.87         |8.86               |2020-02-29     |2019-11-30   |\n",
      "+----------+-----------------------+-------------+-------------------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"datediff_date\", round(months_between(current_date, $\"date\"), 2)).\n",
    "    withColumn(\"months_between_time\", round(months_between(current_timestamp, $\"time\"), 2)).\n",
    "    withColumn(\"add_months_date\", add_months($\"date\",3)).\n",
    "    withColumn(\"add_time_date\", add_months($\"time\",3)).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date and Time - trunc and date_trunc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Data Warehousing we quite often run to date reports such as week to date, month to date, year to date etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can use `trunc` or `date_trunc` for the same to get the beginnig date of the week, month, current_year etc by passing date or timestamp to it.\n",
    "- We can use `trunc` to get the beginning date of the month or year by passing date or timestamp to it \n",
    " - for example: `trunc(current_date(), \"MM\")` will give the first of the current month.\n",
    "- We can use `date_trunc` to get the beginning date of the month or year as well as beginning time of the day or hour by passing timestamp to it.\n",
    " - Get beginning date based on month - `date_trunc(\"MM\", current_timestamp())`\n",
    " - Get beginning based on day - `date_trunc(\"DAY\", cutrent_timestmap())`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetimes = List((2014-02-28,2014-02-28 10:00:00.123), (2016-02-29,2016-02-29 08:08:08.999), (2017-10-31,2017-12-31 11:59:59.123), (2019-11-30,2019-08-31 00:00:00.000))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "List((2014-02-28,2014-02-28 10:00:00.123), (2016-02-29,2016-02-29 08:08:08.999), (2017-10-31,2017-12-31 11:59:59.123), (2019-11-30,2019-08-31 00:00:00.000))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datetimes= List((\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "           (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "           (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "           (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetimesDF = [date: string, time: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[date: string, time: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datetimesDF = datetimes.toDF(\"date\", \"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|date      |time                   |\n",
      "+----------+-----------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|\n",
      "+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+----------+----------+\n",
      "|date      |time                   |date_trunc|year_trnc |\n",
      "+----------+-----------------------+----------+----------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014-02-01|2014-01-01|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016-02-01|2016-01-01|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017-10-01|2017-01-01|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019-11-01|2019-01-01|\n",
      "+----------+-----------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"date_trunc\", trunc($\"date\", \"MM\")).\n",
    "    withColumn(\"year_trnc\", trunc($\"time\", \"yyyy\")).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.date_trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------------+-------------------+\n",
      "|date      |time                   |date_dt            |time_dt            |\n",
      "+----------+-----------------------+-------------------+-------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014-02-28 00:00:00|2014-02-24 00:00:00|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016-02-29 00:00:00|2016-02-29 00:00:00|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017-10-31 00:00:00|2017-12-25 00:00:00|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019-11-30 00:00:00|2019-08-26 00:00:00|\n",
      "+----------+-----------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"date_dt\", date_trunc(\"HOUR\", $\"date\")).\n",
    "    withColumn(\"time_dt\", date_trunc(\"WEEK\", $\"time\")).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date and Time - Extracting Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us understand how to extract information from dates or timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can use date_format to extract the required information in a desired format from date or timestamp\n",
    "- There are also specific functions to extract year, month, day with in a week, a day with in a month, day with in a year etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+---------+---------+\n",
      "|date      |time                   |date_year|time_year|\n",
      "+----------+-----------------------+---------+---------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014     |2014     |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016     |2016     |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017     |2017     |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019     |2019     |\n",
      "+----------+-----------------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"date_year\", year($\"date\")).\n",
    "    withColumn(\"time_year\", year($\"time\")).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+---------+---------+\n",
      "|date      |time                   |date_time|time_time|\n",
      "+----------+-----------------------+---------+---------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2        |2        |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2        |2        |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|10       |12       |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|11       |8        |\n",
      "+----------+-----------------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"date_time\", month($\"date\")).\n",
    "    withColumn(\"time_time\", month($\"time\")).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------+-------+\n",
      "|date      |time                   |date_ym|time_tm|\n",
      "+----------+-----------------------+-------+-------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|201402 |201402 |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|201602 |201602 |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|201710 |201712 |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|201911 |201908 |\n",
      "+----------+-----------------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"date_ym\", date_format($\"date\", \"yyyyMM\")).\n",
    "    withColumn(\"time_tm\", date_format($\"time\", \"yyyyMM\")).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "l = List(x)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "List(x)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val l = List(\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [dummy: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[dummy: string]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = l.toDF(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{dayofweek, current_date, dayofmonth, dayofyear}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|dayofweek(current_date())|\n",
      "+-------------------------+\n",
      "|                        3|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(dayofweek(current_date)).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+--------+--------+--------+--------+--------+--------+\n",
      "|date      |time                   |date_dow|time_dow|date_dom|time_dom|date_doy|time_doy|\n",
      "+----------+-----------------------+--------+--------+--------+--------+--------+--------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|6       |6       |28      |28      |59      |59      |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2       |2       |29      |29      |60      |60      |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|3       |1       |31      |31      |304     |365     |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|7       |7       |30      |31      |334     |243     |\n",
      "+----------+-----------------------+--------+--------+--------+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"date_dow\", dayofweek($\"date\")).\n",
    "    withColumn(\"time_dow\", dayofweek($\"time\")).\n",
    "    withColumn(\"date_dom\", dayofmonth($\"date\")).\n",
    "    withColumn(\"time_dom\", dayofmonth($\"time\")).\n",
    "    withColumn(\"date_doy\", dayofyear($\"date\")).\n",
    "    withColumn(\"time_doy\", dayofyear($\"time\")).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get the information from time in yyyyMMddHHmmss format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+------------------+\n",
      "|date      |time                   |time_ts           |\n",
      "+----------+-----------------------+------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014 02 28 10 0000|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016 02 29 08 0808|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017 12 31 11 5959|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019 08 31 00 0000|\n",
      "+----------+-----------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"time_ts\", date_format($\"time\", \"yyyy MM dd HH mmss\")).\n",
    "show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+----------+-----------+\n",
      "|date      |time                   |date_us   |date_f     |\n",
      "+----------+-----------------------+----------+-----------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|02-28-2014|28-Feb-2014|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|02-29-2016|29-Feb-2016|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|10-31-2017|31-Oct-2017|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|11-30-2019|30-Nov-2019|\n",
      "+----------+-----------------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"date_us\", date_format($\"date\", \"MM-dd-yyyy\")).\n",
    "    withColumn(\"date_f\", date_format($\"date\", \"dd-MMM-yyyy\")).\n",
    "show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with UNIX Timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is an integer and started from January 1st 1970 Midnight UTC.\n",
    "- Beginning time is also known as epoch and is incremented by 1 every second.\n",
    "- We can convert UNIX Timestamp to regular date ot timestamp to a UNIX timestamp value and vice a versa.\n",
    "- We can use `unix_timestamp` to convert regular date ot timestamp to a UNIX timestamp value.\n",
    "- For example:\n",
    " - `unix_timestamp(lit(\"2019-11-19 00:00:00\"))`\n",
    "- We can use `from_unixtime` to convert unix timestamp to regular date or timestamp.\n",
    "- for example - \n",
    " - `from_unixtime(lit(1574101800))`\n",
    "- We can also pass format for both the format to both the functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetimes = List((20140228,2014-02-28,2014-02-28 10:00:00.123), (20160229,2016-02-29,2016-02-29 08:08:08.999), (20171031,2017-10-31,2017-12-31 11:59:59.123), (20191130,2019-11-30,2019-08-31 00:00:00.000))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "List((20140228,2014-02-28,2014-02-28 10:00:00.123), (20160229,2016-02-29,2016-02-29 08:08:08.999), (20171031,2017-10-31,2017-12-31 11:59:59.123), (20191130,2019-11-30,2019-08-31 00:00:00.000))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datetimes= List((\"20140228\", \"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "           (\"20160229\",\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "           (\"20171031\",\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "           (\"20191130\",\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetimesDF = [date_id: string, date: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[date_id: string, date: string ... 1 more field]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datetimesDF = datetimes.toDF(\"date_id\",\"date\", \"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------------+\n",
      "|date_id |date      |time                   |\n",
      "+--------+----------+-----------------------+\n",
      "|20140228|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|20160229|2016-02-29|2016-02-29 08:08:08.999|\n",
      "|20171031|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|20191130|2019-11-30|2019-08-31 00:00:00.000|\n",
      "+--------+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.unix_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get unix timestamp for dateid, date and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------------+----------+-----------+----------+\n",
      "|date_id |date      |time                   |unix_time |unix_dateid|unix_date |\n",
      "+--------+----------+-----------------------+----------+-----------+----------+\n",
      "|20140228|2014-02-28|2014-02-28 10:00:00.123|1393599600|1393563600 |1393563600|\n",
      "|20160229|2016-02-29|2016-02-29 08:08:08.999|1456751288|1456722000 |1456722000|\n",
      "|20171031|2017-10-31|2017-12-31 11:59:59.123|1514739599|1509422400 |1509422400|\n",
      "|20191130|2019-11-30|2019-08-31 00:00:00.000|1567224000|1575090000 |1575090000|\n",
      "+--------+----------+-----------------------+----------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"unix_time\", unix_timestamp($\"time\")).\n",
    "    withColumn(\"unix_dateid\", unix_timestamp($\"date_id\", \"yyyyMMdd\")).\n",
    "    withColumn(\"unix_date\", unix_timestamp($\"date\", \"yyyy-MM-dd\")).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unixTimes = List(1393563600, 1456722000, 1509422400, 1575090000)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "List(1393563600, 1456722000, 1509422400, 1575090000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val unixTimes = List(1393563600,\n",
    "                     1456722000,\n",
    "                     1509422400,\n",
    "                     1575090000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unxitimesDF = [unixtime: int]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[unixtime: int]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val unxitimesDF = unixTimes.toDF(\"unixtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  unixtime|\n",
      "+----------+\n",
      "|1393563600|\n",
      "|1456722000|\n",
      "|1509422400|\n",
      "|1575090000|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unxitimesDF.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.from_unixtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|unixtime  |time               |\n",
      "+----------+-------------------+\n",
      "|1393563600|2014-02-28 00:00:00|\n",
      "|1456722000|2016-02-29 00:00:00|\n",
      "|1509422400|2017-10-31 00:00:00|\n",
      "|1575090000|2019-11-30 00:00:00|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unxitimesDF.\n",
    "    withColumn(\"time\", from_unixtime($\"unixtime\", \"yyyy-MM-dd\")).\n",
    "withColumn(\"time\", from_unixtime($\"unixtime\")).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
