{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing - Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites and Module Introduction\n",
    "\n",
    "Let us understand prerequisites before getting into the module.\n",
    "* Good understanding of Data Processing using Scala.\n",
    "* Data Processing Life Cycle\n",
    " * Reading Data from files\n",
    " * Processing Data using APIs\n",
    " * Writing Processed Data back to files\n",
    "* We can also use Databases as sources and sinks. It will be covered in relevant modules.\n",
    "* We can also read data in streaming fashion which is out of the scope of this course.\n",
    "\n",
    "\n",
    "We will get an overview of the Data Processing Life Cycle by the end of the module.\n",
    "* Read data from the file.\n",
    "* Preview the schema and data to understand the characteristics of the data.\n",
    "* Get an overview of Data Frame APIs as well as functions used to process the data.\n",
    "* Check if there are any duplicates in the data.\n",
    "* Get an overview of how to write data in Data Frames to Files using File Formats such as Parquet using Compression.\n",
    "* We will deep dive into Data Frame APIs to process the data in subsequent modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Spark Context\n",
    "\n",
    "Let us understand more about Spark Context and also how to start it using SparkSession.\n",
    "\n",
    "* `SparkSession` is a class that is part of `org.apache.spark.sql` package.\n",
    "* When Spark application is submitted using `spark-submit` or `spark-shell` or `pyspark`, a web service called as Spark Context will be started.\n",
    "* Here is the example about how Spark Shell can be launched locally.\n",
    "```\n",
    "spark-shell \\\n",
    "    --master \"local[*]\"\n",
    "```\n",
    "* Here is the example about how Spark Shell can be launched on multinode cluster such as our labs.\n",
    "```\n",
    "spark2-shell \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0\n",
    "```\n",
    "* **Make sure to understand the enviroment and use appropriate command to launch Spark Shell.**\n",
    "* Spark Context maintains the context of all the jobs that are submitted until it is killed.\n",
    "* `SparkSession` is nothing but wrapper on top of Spark Context.\n",
    "* We need to first create SparkSession object with any name. But typically we use `spark`. Once it is created, several APIs will be exposed including `read`.\n",
    "* We need to at least set Application Name and also specify the execution mode in which Spark Context should run while creating `SparkSession` object.\n",
    "* We can use `appName` to specify name for the application and `master` to specify the execution mode.\n",
    "* Below is the sample code snippet which will start the Spark Session object for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val spark = SparkSession.\n",
    "    builder.\n",
    "    config(\"spark.ui.port\", \"0\").\n",
    "    appName(\"Data Processing - Overview\").\n",
    "    master(\"yarn\").\n",
    "    getOrCreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.getConf.getAll.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Spark read APIs\n",
    "\n",
    "Let us get the overview of Spark read APIs to read files of different formats.\n",
    "\n",
    "* `spark` has a bunch of APIs to read data from files of different formats.\n",
    "* All APIs are exposed under `spark.read`\n",
    " * `text` - to read single column data from text files as well as reading each of the whole text file as one record.\n",
    " * `csv`- to read text files with delimiters. Default is a comma, but we can use other delimiters as well.\n",
    " * `json` - to read data from JSON files\n",
    " * `orc` - to read data from ORC files\n",
    " * `parquet` - to read data from Parquet files.\n",
    " * We can also read data from other file formats by plugging in and by using `spark.read.format`\n",
    "* We can also pass options based on the file formats. Go to Scala Docs which will be provided as part of the certification exam to get the list of options available.\n",
    " * `inferSchema` - to infer the data types of the columns based on the data.\n",
    " * `header` - to use header to get the column names in case of text files.\n",
    " * `schema` - to explicitly specify the schema.\n",
    "* Let us see an example about how to read delimited data from text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// spark.read.csv\n",
    "val orders = spark.\n",
    "    read.\n",
    "    schema(\"\"\"order_id INT, order_date TIMESTAMP,\n",
    "              order_customer_id INT, order_status STRING\n",
    "           \"\"\").\n",
    "    csv(\"/public/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// spark.read.csv with option\n",
    "\n",
    "val orders = spark.\n",
    "    read.\n",
    "    schema(\"\"\"order_id INT, order_date TIMESTAMP,\n",
    "              order_customer_id INT, order_status STRING\n",
    "           \"\"\").\n",
    "    option(\"sep\", \",\").\n",
    "    csv(\"/public/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// spark.read.format\n",
    "\n",
    "val orders = spark.\n",
    "    read.\n",
    "    schema(\"\"\"order_id INT, order_date TIMESTAMP,\n",
    "              order_customer_id INT, order_status STRING\n",
    "           \"\"\").\n",
    "    option(\"sep\", \",\").\n",
    "    format(\"csv\").\n",
    "    load(\"/public/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reading JSON data from text files. We can infer schema from the data as each JSON object contain both column name and value.\n",
    "* Example for JSON\n",
    "\n",
    "```\n",
    "{ \"order_id\": 1, \"order_date\": \"2013-07-25 00:00:00.0\", \"order_customer_id\": 12345, \"order_status\": \"COMPLETE\" }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// spark.read.json\n",
    "\n",
    "val orders = spark.\n",
    "    read.\n",
    "    option(\"inferSchema\", \"false\").\n",
    "    schema(\"\"\"order_id INT, order_date TIMESTAMP,\n",
    "              order_customer_id INT, order_status STRING\n",
    "           \"\"\").\n",
    "    json(\"/public/retail_db_json/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// spark.read.format\n",
    "\n",
    "val orders = spark.\n",
    "    read.\n",
    "    option(\"inferSchema\", \"false\").\n",
    "    schema(\"\"\"order_id INT, order_date TIMESTAMP,\n",
    "              order_customer_id INT, order_status STRING\n",
    "           \"\"\").\n",
    "    format(\"json\").\n",
    "    load(\"/public/retail_db_json/orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previewing Schema and Data\n",
    "\n",
    "Here are the APIs that can be used to preview the schema and data.\n",
    "\n",
    "* `printSchema` can be used to get the schema details.\n",
    "* `show` can be used to preview the data. It will typically show first 20 records where output is truncated.\n",
    "* `describe` can be used to get statistics out of our data.\n",
    "* We can pass number of records and set truncate to false while previewing the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val orders = spark.\n",
    "    read.\n",
    "    schema(\"\"\"order_id INT, \n",
    "              order_date STRING, \n",
    "              order_customer_id INT, \n",
    "              order_status STRING\n",
    "           \"\"\"\n",
    "          ).\n",
    "    csv(\"/public/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Print Schema\n",
    "orders.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Describe\n",
    "orders.describe().show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Preview Data - Default\n",
    "orders.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Preview Data - 10, with truncate false\n",
    "orders.show(10, truncate=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Data Frame APIs\n",
    "\n",
    "Let us get an overview of Data Frame APIs to process data in Data Frames.\n",
    "* Row Level Transformations or Projection of Data can be done using `select`, `selectExpr`, `withColumn`, `drop` on Data Frame.\n",
    "* We typically apply functions from `org.apache.spark.sql.functions` on columns using `select` and `withColumn`\n",
    "* Filtering is typically done either by using `filter` or `where` on Data Frame.\n",
    "* We can pass the condition to `filter` or `where` either by using SQL Style or Programming Language Style.\n",
    "* Global Aggregations can be performed directly on the Data Frame.\n",
    "* By Key or Grouping Aggregations are typically performed using `groupBy` and then aggregate functions using `agg`\n",
    "* We can sort the data in Data Frame using `sort` or `orderBy`\n",
    "* We will talk about Window Functions later. We can use use Window Functions for some advanced Aggregations and Ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "Let us understand how to project the data using different options such as `select`, `selectExpr`, `withColumn`, `drop`.\n",
    "\n",
    "* Create Dataframe **employees** using Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val employees = List((1, \"Scott\", \"Tiger\", 1000.0, \"united states\"),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, \"India\"),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, \"united KINGDOM\"),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, \"AUSTRALIA\")\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val employeesDF = employees.\n",
    "    toDF(\"employee_id\", \n",
    "         \"first_name\", \n",
    "         \"last_name\", \n",
    "         \"salary\", \n",
    "         \"nationality\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Project employee first name and last name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF.select(\"first_name\", \"last_name\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Project all the fields except for Nationality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF.drop(\"nationality\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will explore most of the APIs to process data in Data Frames as we get into the data processing at a later point in time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Functions\n",
    "\n",
    "Let us get an overview of different functions that are available to process data in columns.\n",
    "* While Data Frame APIs work on the Data Frame, at times we might want to apply functions on column values.\n",
    "* Functions to process column values are available under `org.apache.spark.sql.functions`. These are typically used in select or withColumn on top of Data Frame.\n",
    "* There are approximately 300 pre-defined functions available for us.\n",
    "* Some of the important functions can be broadly categorized into String Manipulation, Date Manipulation, Numeric Functions and Aggregate Functions.\n",
    "* String Manipulation Functions\n",
    " * Concatenating Strings - `concat`\n",
    " * Getting Length - `length`\n",
    " * Trimming Strings - `trim`,` rtrim`, `ltrim`\n",
    " * Padding Strings - `lpad`, `rpad`\n",
    " * Extracting Strings - `split`, `substring`\n",
    "* Date Manipulation Functions\n",
    " * Date Arithmetic - `date_add`, `date_sub`, `datediff`, `add_months`\n",
    " * Date Extraction - `dayofmonth`, `month`, `year`, `date_format`\n",
    " * Get beginning period - `trunc`, `date_trunc`\n",
    "* Numeric Functions - `abs`, `greatest`\n",
    "* Aggregate Functions - `sum`, `min`, `max`\n",
    "* There are some special functions such as `col`, `lit` etc.\n",
    "  * `col` is used to convert string to column type. It can also be invoked using **$** after importing `spark.implicits._`\n",
    "  * `lit` is used to convert a literal to column value so that it can be used to generate derived fields by manipulating using literals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "Let us perform a task to understand how functions are typically used.\n",
    "\n",
    "* Project full name by concatenating first name and last name along with other fields excluding first name and last name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Using col and lit\n",
    "import org.apache.spark.sql.functions.{col, lit, concat}\n",
    "employeesDF.\n",
    "    select(col(\"employee_id\"),\n",
    "           concat(col(\"first_name\"), lit(\" \"), col(\"last_name\")).alias(\"full_name\"),\n",
    "           col(\"salary\"),\n",
    "           col(\"nationality\")\n",
    "          ).\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF.\n",
    "    withColumn(\"full_name\", \n",
    "               concat(col(\"first_name\"), lit(\", \"), col(\"last_name\"))).\n",
    "    drop(\"first_name\", \"last_name\").\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val spark = SparkSession.\n",
    "    builder.\n",
    "    config(\"spark.ui.port\", \"0\").\n",
    "    appName(\"Data Processing - Overview\").\n",
    "    master(\"yarn\").\n",
    "    getOrCreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Using $ and lit\n",
    "import spark.implicits._\n",
    "employeesDF.\n",
    "    withColumn(\"full_name\", \n",
    "               concat($\"first_name\", lit(\", \"), $\"last_name\")).\n",
    "    drop(\"first_name\", \"last_name\").\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW functions\").show(300, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DESCRIBE FUNCTION concat\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Using SQL Style\n",
    "employeesDF.\n",
    "    selectExpr(\"employee_id\",\n",
    "               \"concat(first_name, ' ', last_name) AS full_name\",\n",
    "               \"salary\", \n",
    "               \"nationality\"\n",
    "              ).\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will explore most of the functions as we get into the data processing at a later point in time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Spark Write APIs\n",
    "\n",
    "Let us understand how we can write Data Frames to different file formats.\n",
    "* All the batch write APIs are grouped under write which is exposed to Data Frame objects.\n",
    "* All APIs are exposed under spark.read\n",
    " * `text` - to write single column data to text files.\n",
    " * `csv` - to write to text files with delimiters. Default is a comma, but we can use other delimiters as well.\n",
    " * `json` - to write data to JSON files\n",
    " * `orc` - to write data to ORC files\n",
    " * `parquet` - to write data to Parquet files.\n",
    "* We can also write data to other file formats by plugging in and by using `write.format`, for example **avro**\n",
    "* We can use options based on the type using which we are writing the Data Frame to.\n",
    " * `compression` - Compression codec (`gzip`, `snappy` etc)\n",
    " * `sep` - to specify delimiters while writing into text files using **csv**\n",
    "* We can `overwrite` the directories or `append` to existing directories using `mode`\n",
    "* Create copy of orders data in **parquet** file format with no compression. If the folder already exists overwrite it. Target Location: **/user/[YOUR_USER_NAME]/retail_db/orders**\n",
    "* When you pass options, if there are typos then options will be ignored rather than failing. Be careful and make sure that output is validated.\n",
    "* By default the number of files in the output directory is equal to number of tasks that are used to process the data in the last stage. However, we might want to control number of files so that we don\"t run into too many small files issue.\n",
    "* We can control number of files by using `coalesce`. It has to be invoked on top of Data Frame before invoking `write`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orders = [order_id: int, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orders = spark.\n",
    "    read.\n",
    "    schema(\"\"\"order_id INT, \n",
    "              order_date STRING, \n",
    "              order_customer_id INT, \n",
    "              order_status STRING\n",
    "           \"\"\"\n",
    "          ).\n",
    "    csv(\"/public/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "snappy"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.parquet.compression.codec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Using write.parquet\n",
    "orders.\n",
    "    write.\n",
    "    mode(\"overwrite\").\n",
    "    option(\"compression\", \"none\").\n",
    "    parquet(\"/user/training/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Using write.format(\"parquet\")\n",
    "orders.\n",
    "    coalesce(1).\n",
    "    write.\n",
    "    mode(\"overwrite\").\n",
    "    option(\"compression\", \"none\").\n",
    "    format(\"parquet\").\n",
    "    save(\"/user/training/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   2 training training          0 2020-04-07 20:56 /user/training/retail_db/orders/_SUCCESS\n",
      "-rw-r--r--   2 training training     495141 2020-04-07 20:56 /user/training/retail_db/orders/part-00000-4e6cfdeb-f1c9-4758-bff3-0fe28ca342f3-c000.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys.process._\n",
    "\n",
    "\"hdfs dfs -ls /user/training/retail_db/orders\" !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"/user/training/retail_db/orders\").printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|             9198|     PROCESSING|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"/user/training/retail_db/orders\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Let us recap about key takeaways from this module.\n",
    "* APIs to read the data from files into Data Frame.\n",
    "* Previewing Schema and the data in Data Frame.\n",
    "* Overview of Data Frame APIs and Functions\n",
    "* Writing data from Data Frame into Files\n",
    "* Reorganizing the airlines data by month\n",
    "* Simple APIs to analyze the data.\n",
    "\n",
    "Now it is time for us to deep dive into APIs to perform all the standard transformations as part of Data Processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
